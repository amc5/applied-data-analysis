---
title: "Module 08"
#output:
#    github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	comment = "##",
	prompt = TRUE,
	tidy = TRUE,
	tidy.opts = list(width.cutoff = 75),
	fig.path = "img/"
)
```
# Statistical Inference 1 - Probabilities and Distributions

## Objectives
> The objective of this module to begin our discussion of statistical inference from a frequentist statistics approach. Doing so means that we need to cover basics of probability and distributions

## Important Terms and Concepts to Reiterate

- **Population** = includes **all** of the elements from a set of data = ***N***
- **Sample** = one or more observations from a population = ***n***
- **Parameter** = a measurable characteristic of a *population*
- **Statistic** = a measureable characteristic about a *sample*

When we do **statistical inference** we are basically trying to draw conclusions about a *population* based on measurements from a noisy *sample* or we are trying to evaluate whether it is reasonable to assume that our sample is drawn from a particular population.

This process of trying to draw conclusions is complicated by the fact that...

- our sample may be biased, non-random, or non-representative in some way
- there may be unknown or unobserved variables that impact how the sample is related to the population
- the assumptions we make about the population that our sample is drawn from might not be correct

### PROBABILITY

The term **PROBABILITY** is applied to a population level variables that describe the magnitude of chance associated with a particular observation or event. Probabilities summarize the relative frequencies of possible outcomes of an experiment.

Example: if we roll a (fair) die, there are 6 possible outcomes, each has a probability of occurring of 1 in 6. This is referred to as a *frequentist* or *classical* way of thinking about the probability of different outcomes... the relative frequency with which an event occurs over numerous identical, objective trials.

Simulating die rolling with the `sample()` function... let's play with changing the number of rolls.

``` {r eval = FALSE}
library(manipulate)
outcomes <- c(1,2,3,4,5,6)
manipulate(
	hist(sample(outcomes, n, replace = TRUE),
			 breaks=c(0.5,1.5,2.5,3.5,4.5,5.5,6.5),
			 probability = TRUE,
			 main = paste("Histogram of Outcomes of ", n, " Die Rolls", sep=""),
			 xlab = "roll",
			 ylab = "probability"),
	n=slider(0, 10000, initial=100, step=100)
)
```
#### CHALLENGE:
Write a function to simulate die rolling where you pass the number of rolls as an argument. Then, use your function to simulate rolling two dice 1000 times and take the sum of the rolls. Plot a histogram of those results.

``` {r}
roll <- function(x) {sample(1:6, x, replace = TRUE)}
two_dice <- roll(2000)+roll(2000)
hist(two_dice,breaks=c(1.5:12.5), probability = TRUE, main="Rolling Two Dice", xlab = "sum of rolls", ylab = "probability")
```

### RULES OF PROBABILITY

1. $Pr$ (+) = Probability that something occurs = 1
2. $Pr$ ($\emptyset$) = Probability that nothing occurs = 0
3. $Pr$ (A) = Probability that a particular event, A, occurs

	$Pr$ (A) $\geq$ 0 and $Pr$ (A) $\leq$ 1

4. $Pr$ (A $\cup$ B) = Probability that particular event A *or* event B occurs = **UNION**

	$Pr$ (A $\cup$ B) = $Pr$(A) + $Pr$(B) - $Pr$ (A $\cap$ B)

	If A and B are mutually exclusive, then this simplifies to $Pr$ (A) + $Pr$ (B)

5. $Pr$ (A $\cap$ B) = Probability that both A *and* B occur simultaneously = **INTERSECTION**.

	$Pr$ (A $\cap$ B)  = $Pr$ (A|B) × $Pr$ (B) = $Pr$ (B|A) × $Pr$ (A)

	Where the pipe operator ("|") can be read as "given".
	
	If the 2 events are *independent* (i.e., the probability of one does not depend on the probability of the other), then $Pr$ (A $\cap$ B) simplifies to $Pr$ (A) × $Pr$ (B).
	
#### CHALLENGE:
What is the probability that a card drawn at random from a deck is both from a red suit (hearts or diamond) (2/4) and a face card (12/52)?

$Pr$ (A) = 1/2

$Pr$ (B) = 12/52

Pr A $\cap$ B = $Pr$ (A|B) (0.5) $\times$ $Pr$ (B) (12/52)

If $Pr$ (A $\cap$ B) = 0, then we say the events are *mutually exclusive* (e.g., you cannot have a die roll be 1 *and* 2)

6. $Pr$ ($\bar A$) = Probabilty of the complement of A (i.e., *not* A) = 1 - $Pr$ (A)


**CONDITIONAL PROBABILITY** - probability that one event occurs after taking into account the occurrence of another event. I.e., a second event is *conditioned* on the occurrence of a first event, e.g., the probability of a die coming up as a "1" given that we know the die came up as "odd".

$Pr$ (A|B) = $Pr$ (A $\cap$ B) / $Pr$ (B)

If A and B are *independent*, then $Pr$ (A|B) = ($Pr$ (A) × $Pr$ (B)) / $Pr$ (B) = $Pr$ (A)
If A and B are *dependent* then $Pr$ (A|B) ≠ $Pr$ (A)

**RANDOM VARIABLE** -- a variable whose outcomes are assumed to arise by chance or according to some random or stochastic mechanism; the chances of observing a specific value or a value within a specific interval for this random variable has associated with it a probability.

Random variables come in two varieties:

*Discrete Random Variables* - random variables that can assume only a countable number of discrete possibilities (e.g., counts of occurrences in a particular category); we can assign probabilities to the occurence of each value

#### CHALLENGE:
You have a deck of 52 cards, Ace to 10 + 3 face cards in each suit. You draw a card at random.

- What is the probability that you draw a face card?
- What is the probability that you draw a King?
- What is the probability that you draw a spade?
- What is the probability that you draw a space given that you draw a face card?
- What is the probability that you draw a King given that you draw a face card?

*Continuous Random Variables* - random variables that can assume any real number value within a given range (e.g., measurements). We cannot assign probabilities to the occurrence of each value, but we can assign probabilites to intervals of values.

With these basics in mind, we can define a few more terms:

### PROBABILITY FUNCTION
A **PROBABILITY FUNCTION** is a mathematical function that describes the chance associated with a random variable having a particular value or falling within a given range of values.

As for kinds of variable, we can distinguish two types of probability functions:

*Probability Mass Functions (PMF)* - associated with discrete random variables; describe the probability that a random variable takes a particular value

To be a valid *PMF*, a function $f(x)$ must satisfy the following:

  1. There are $k$ distinct outcomes $x_1, x_2,...,x_k$
  2. $Pr(X=x_i)$ $\geq$ 0 and $\leq$ 1 for all $x_i$
  3. $\sum_{i=1}^k Pr(X=x_i)$ = 1

Example:
``` {r}
outcomes <- c("heads","tails")
prob <- c(1/2,1/2)
barplot(prob,ylim=c(0,0.6),names.arg=outcomes,space=0.1,
           xlab="x",ylab="Pr(X = x)")
cumprob <- cumsum(prob)
barplot(cumprob,names.arg=outcomes,space=0.1,main = "Cumulative Probability", xlab="x",ylab="Pr(X <= x)")

outcomes <- c(1,2,3,4,5,6)
prob <- c(1/6,1/6,1/6,1/6,1/6,1/6)
barplot(prob,ylim=c(0,0.5),names.arg=outcomes,space=0.1,
           xlab="x",ylab="Pr(X = x)")

cumprob <- cumsum(prob)

barplot(cumprob,names.arg=outcomes,space=0.1,main = "Cumulative Probability",xlab="x",ylab="Pr(X <= x)")
```

*Probability Density Function (PDF)* - associated with continuous random variables; describes the probability that a random variable falls within a given range of values

To be a valid *PDF*, a function $f(x)$ must satisfy

1. $f(x) \geq 0$ for all $-\infty\leq x \leq+\infty$
2. $\int_{-\infty}^{+\infty} f(x) dx$ = 1. That is, the total area under $f(x)$ = 1

Example:
The **Beta Distribution** refers to a family of continuous probability distributions defined over the interval [0, 1], parametrized by two positive shape parameters, denoted by α and β, that appear as exponents of the random variable and control the shape of the distribution.

$f(x)=Kx^{\alpha-1}(1-x)^{\beta-1}$

Let's set $K$ = 2, $\alpha$ = 2, and $\beta$ = 1 and restrict the domain of $x$ to [0, 1]. This will give us a triangular function that we can graph as follows:

``` {r eval = FALSE}
library(ggplot2)
a <- 2
b <- 1
K <- 2
x <- seq(from= 0, to=1, by = 0.025)
y <- K*x^(a-1)*(1-x)^(b-1)
lowerx <- seq(from=-0.25, to=0, by= 0.025)
upperx <- seq(from=1, to=1.25, by= 0.025)
lowery <- rep(0,11)
uppery <- rep(0,11)
x<-c(lowerx,x,upperx)
y<-c(lowery,y,uppery)
d<-as.data.frame(cbind(x,y))
p <- ggplot(data = d, aes(x=x, y=y), xlab = "x", ylab = "y") + geom_line()
p
```
Is this a *PDF*? Why or why not?

Yes... the total area under the function is 1.

We can explore this interactively with the code below:
``` {r eval = FALSE}
library(manipulate)
manipulate({
	ggplot(data = d, aes(x=x, y=y), xlab = "x", ylab = "y") +
		geom_line() +
		geom_polygon(data=data.frame(xvals=c(0,n,n,0),yvals=c(0,K*n^(a-1)*(1-n)^(b-1),0,0)), aes(x=xvals,y=yvals)) +
		ggtitle(paste("Area Under Function = ", 0.5 * n * K*n^(a-1)*(1-n)^(b-1),sep=" "))
		},
  #define variable that will be changed in plot
    n=slider(0, 1, initial=0.5, step=0.01)
)
```

The shaded area here represents the **cumulative probability distribution**.

The **cumulative distribution function**, or CDF, of a random variable is defined as the probability of observing a random variable $X$ taking the value of $x$ or less, i.e., $f(x) = Pr (X \leq x)$

- This definition applies regardless of whether $X$ is discrete or continuous.

``` {r eval = FALSE}
x <- seq(from=0, to=1, by=0.005)
prob <- 0.5 * x * K*x^(a-1)*(1-x)^(b-1)
barplot(prob,names.arg=x,space=0,main = "Cumulative Probability", xlab="x",ylab="Pr(X <= x)")
```

The built in ***R*** function for the beta distribution, `pbeta()`, can give us this directly, if we specify the values of $\alpha$ = 2 and $\beta$ = 1.

``` {r}
pbeta(0.75, 2, 1)
pbeta(0.5, 2, 1)
```

In general, we find the cumulative probability for a continuous random variable by calculating the area under the density function of interest from −$\infty$ to $x$. This is what is returned from `pbeta()`. The other related functions for the *Beta Distribution** -- `rbeta()`, `dbeta`, and `qbeta()` -- are also useful. `rbeta()` draws random observations from the specfied beta distribution. `dbeta()` gives the point estimate of the density function at the value of the argument, and `qbeta()` is essentially the converse of `pbeta()` -- it tells you the value of $x$ that is associated with a particular proportion, or quantile, of the cumulative distribution function.

``` {r eval =FALSE}
dbeta()
pbeta (0.7,2,1) # yields .49
qbeta (0.49,2,1) # yield 0.7
dbeta (0.7) # yields 1.4
```

Finally, we can define the **survival function** of a random variable $X$ as $S(x) = Pr(X > x)$ = 1 - $f(x)$

## Mean and Variance of Random Variables

The mean (or expected) value for a random varible with a given probability mass function can be expressed generally as follows:

$\mu_X$ = Expectation of $X$ = $\sum_{i=1}^k x_i\times Pr(X=x_i)$

$\sigma_X^2$ = Variance = $\sum_{i=1}^k (x_i - \mu_X)^2 \times Pr (X=x_i)$

Applying these formulae to die rolls, we could calculate the expectation (i.e., mean) of a large set of die rolls...

(1 x 1/6) + (2 x 1/6) + ... + (6 x 1/6) = 3.5

``` {r}
m <-sum(seq(1:6) * 1/6)
m
```

And the expected variance... 

(1 - 3.5)^2 + (2 - 3.5)^2 + ... + (6 - 3.5)^2 = 
``` {r}
var <- sum((1/6)*(seq(1:6)-mean(seq(1:6)))^2)
var
```

Likewise, we can calculate the expected mean and variance for a random varible with a given probability density function as follows:

$\mu_X$ = Expectation of $X$ = $\int_{-\infty}^{+\infty}$ $x$ $f(x)$ $dx$

$\sigma_X^2$ = Variance in $X$ = $\int_{-\infty}^{+\infty} (x - \mu_X)^2$ $f(x)$ $dx$

To demonstrate these numerically would require a bit of calculus -- i.e., integration.


## Useful Probability Distributions for Modeling Random Variables

### Probability Mass Functions

#### The Bernoulli Distribution

The **Bernoulli Distribution** is the probability distribution of a **BINARY** random variable -- i.e., a variable that has only two possible outcomes, such as success or failure, heads or tails, true or false. If $p$ is the probability of one outcome, then $1-p$ has to be the probabilty of the alternative. For flipping a fair coin, for example, $p$ = 0.5 and $1-p$ also = 0.5.

For the **BERNOULLI DISTRIBUTION**, the probability mass function is:

$f(x) = p^x(1-p)^{1-x}$ where $x$ = {0 or 1}

For this distribution, $\mu_X$ = $p$ and $\sigma_X^2$ = $p(1-p)$

#### CHALLENGE:

Using the Bernoulli distribution, what is the expectation for drawing a spade from a deck of cards? What is the variance in this expectation across a large number of draws?

$Pr$ (spade) = $(13/52)^1$ $\times$ $(39/52)^0$ = 0.25

$Var$ (spade) = $(13/52)$ $\times$ $(1-13/52)$ = (0.25) $\times$ (0.75) = 0.1875

#### Binomial Distribution

The Bernoulli distribution is a special case of the **Binomial Distribution**. The binomial distribution is typically used to model the probability of a number of "successes" *n* out of a set of "trials" *p*, i.e., for *counts* of a particular outcome.

Again, the probability of success on each trial = $p$ and the probability of not success = $1-p$.

For the **BINOMIAL DISTRIBUTION**, the probability mass function is:

$f(x) = \binom{n}{x} p^x(1-p)^{n-x}$ where $x$ = {0, 1, 2, ... , n}

and where $\binom{n}{x}$ = $\frac{n!}{x!(n-x)!}$

This is read as "$n$ choose $x$", i.e., the probability of $x$ successes out of $n$ trials.

For this distribution, $\mu_X$ = $np$ and $\sigma_X^2$ = $np(1-p)$

Where $n$ = 1, this simplifies to the Bernoulli distribution.

#### CHALLENGE

What is the chance of getting a "1" on each of six consecutive rolls of a die? What about of getting exactly three "1"s?

```{r}
nrolls <- 6
nsuccesses <- 6
p <- 1/6
prob <- (factorial(nrolls)/(factorial(nsuccesses)*factorial(nrolls-nsuccesses)))*(p^nsuccesses)*(1-p)^(nrolls-nsuccesses)
nrolls <- 6
nsuccesses <- 3
prob <- (factorial(nrolls)/(factorial(nsuccesses)*factorial(nrolls-nsuccesses)))*(p^nsuccesses)*(1-p)^(nrolls-nsuccesses)
```

As for other distributions, ***R*** has a built in function you can use to solve for the mass function probability at a given value, i.e., $Pr (X = x)$.

``` {r}
dbinom(x=nsuccesses,size=nrolls,prob=p)
```

We can also use the built in function `pbinom()` to return the value of the **cumulative distribution function**, i.e., the probability of observing up to and including $X$ successes in $n$ trials.

So, for example, the chances of observing exactly 0, 1, 2, 3, ... 6 rolls of "1" on 6 rolls of a die are...

``` {r}
probset<-dbinom(x=0:6,size=6,prob=1/6)
barplot(probset,names.arg=0:6,space=0,xlab="x",ylab="Pr(X = x)")
sum(probset) # equals 1, as it should
```
The chance of observing exactly 3 rolls of 1 is...
```
dbinom(x=3,size=6,prob=1/6)
```
And the chance of observing up to and including 3 rolls of "1" is...
```{r}
sum(dbinom(x=0:3,size=6,prob=1/6)) # this sums the probabilities of 0, 1, 2, and 3 successes
pbinom(q=3,size=6,prob=1/6) # note the name of the argument is q not x
```

#### Poisson Distribution

The **Poisson Distribution** is often used to model open ended counts of independently occuring events, for example the number of cars that pass a traffic light over a given interval of time or the number of times a monkey scratches itself during a given observation interval. The probability mass function for the Poisson Distribution is described by a single parameter, $\lambda$, where $\lambda$ can be interpreted as the mean number of occurrences in the interval. 

The probability mass function for the **POISSON DISTRIBUTION** is:

$f(x) = \frac{\lambda^xexp(-\lambda)}{x!}$ where $x$ = {0, 1, 2, ...}

For this distribution, $\mu_X$ = $\lambda$ and $\sigma_X^2$ = $\lambda$

Note that the mean and the variance are the same!

Let's use ***R*** to look at the probability mass functions for different values of $\lambda$:

``` {r}
x <- 0:10
l = 3.5
probset<-dpois(x=x,lambda=l)
barplot(probset,names.arg=x,space=0,xlab="x",ylab="Pr(X = x)")
x <- 0:20
l = 10
probset<-dpois(x=x,lambda=l)
barplot(probset,names.arg=x,space=0,xlab="x",ylab="Pr(X = x)")
x <- 0:50
l = 20
probset<-dpois(x=x,lambda=l)
barplot(probset,names.arg=x,space=0,xlab="x",ylab="Pr(X = x)")
```

As we did for other distibutions, we can also use the built in function `ppois()` to return the value of the **cumulative distribution function**, i.e., the probability of observing up to and including $X$ events occurring in the given interval.

```{r}
x <- 0:10
l <- 3.5
barplot(ppois(q=x,lambda=l),ylim=0:1,space=0, names.arg=x,ylab="Pr(X<=x)",xlab="x")
x <- 0:20
l <- 10
barplot(ppois(q=x,lambda=l),ylim=0:1,space=0, names.arg=x,ylab="Pr(X<=x)",xlab="x")
x <- 0:50
l <- 20
barplot(ppois(q=x,lambda=l),ylim=0:1,space=0, names.arg=x,ylab="Pr(X<=x)",xlab="x")
```

#### HOMEWORK:
Create a new ***GitHub*** repository and ***R*** Project named "homework-week-4". In your repo, create an **R Markdown** file and answer the following problems. When you are done, "knit" your **R Markdown** file to `.html` and push your `.Rmd` and `.html` files up to ***GitHub*** for me to look at.

Problem 1:

Every Saturday, at the same time, a primatologist goes and sits in the forest in the morning and listens for gibbon calls, counting the number of calls they hear in a 2 hour window from 5am to 7am. Based on previous knowledge, she believes that the mean number calls she will hear in that time is exactly 15. Let X represent the appropriate Poisson random variable of the number of calls heard in each monitoring session.

- What is the probability that she will hear more than 8 calls during any given session?
- What is the probability that she will hear no calls?
- What is the probability that she will hear exactly 3 calls?
- Plot the relevant Poisson mass function over the values in range 0 ≤ x ≤ 30.
- Simulate 104 results from this distribution (2 years of Saturday monitoring sessions). Plot the simulated results using `hist()`; use `xlim()` to set the horizontal limits from 0 to 30. Compare your histogram to the shape of your mass function from above.

### Probability Density Functions

#### Uniform Distribution

The **Uniform Distribution** is the simplest density function describing a continuous random variable where there is no fluctuation in probability across some interval.

The probability density function for the **UNIFORM DISTRIBUTION** is:

$f(x) = \frac{1}{b-a}$ where $a$ $\leq$ $x$ $\leq$ $b$ and 0 for $x$ < $a$ and $x$ > $b$

What would the expectation (mean) be for a uniform distribution? What about the variance?

For this distribution $\mu_X$ = $\frac{b-a}{2}$ and $\sigma_X^2$ = $\frac{(b-a)^2}{12}$

Plotting the uniform distribution
``` {r}
a <- 4
b <- 8
x <- seq(from=a-(b-a), to=b+(b-a), by=0.01)
y <- dunif(x,min=a, max=b) # dunif() evaluates the density at each x
plot(x,y, type="l")
quant <- seq(from=a, to=b, by = 0.1)
punif(q=quant,min=a,max=b) # punif() is the cumulative probability density up to a given x
plot(quant,punif(q=quant,min=a,max=b)) # the CDF increases linearly
```

#### CHALLENGE:
Simulate a sample of 100000 random numbers from a uniform distribution in the interval between a = 6 and b = 8. Calculate the mean and variance of this sample and compare it to the expectation for these parameters, i.e., $\mu_X$ = $\frac{b-a}{2}$ and $\sigma_X^2$ = $\frac{(b-a)^2}{12}$.

#### Normal Distribution

The **Normal or Gaussian Distribution** is perhaps the most familiar and most commonly applied probability distributions for modeling continuous random variables. Two parameters, $\mu$ and $\sigma$, are used to describe a normal distribution.

We can get an idea of the shape of a normal distribution with different $\mu$ and $\sigma$ as follows using the ***R**** code below. Type it in and then  play with values of mu and sigma.

Remember the function, `dnorm()` gives the value of the density function at a given value of $x$. $x$ can range from -$\infty$ to +$\infty$. Remember, it does not make sense to talk about the "probability" of a given value of $x$ as these are density not mass functions, but we can talk about the probability of $x$ falling within a given interval.

``` {r}
mu <- 6
sigma <- 2
x <- seq(from=(mu - 4*sigma), to=(mu + 4*sigma), length.out = 1000)
fx <- dnorm(x,mean=mu,sd=sigma)
plot(x,fx, type="l") # plots normal distribution with mean = 0 and SD = 1
```

The `pnorm()` function, as with the `p` function for other distributions, returns the cumulative probability of observing a value less than or equal to $x$, i.e., $Pr$ ($X$ $\leq$ $x$) 

``` {r}
CDF<-pnorm(x,mean=mu,sd=sigma)
plot(x,CDF, type="l") # plots the cumulative distribution function
```

You can use `pnorm()` to calculate the probability of observation's value falling within a particular interval. For example, for a normal distribution with $\mu$=6 and $\sigma$=2, the probability of an random observation falling between 7 and 8 is...

``` {r}
prob<-pnorm(8,mean=mu,sd=sigma)-pnorm(7,mean=mu,sd=sigma)
prob
```

Likewise, the probability that an observation falls within 2 standard deviations of the mean a particular normal distribution is...

``` {r}
prob<-pnorm(mu + 2*sigma, mean=mu, sd=sigma) -
	pnorm(mu - 2*sigma, mean=mu, sd=sigma)
prob
```
So... about 95% of the normal distribution falls within 2 standard deviations of the mean.

And about 68% of the distribution falls within 1 standard deviation.

``` {r}
prob<-pnorm(mu + 1*sigma, mean=mu, sd=sigma) -
	pnorm(mu - 1*sigma, mean=mu, sd=sigma)
prob
```

#### R Code for Plotting Normal Distributions

Type this in and play with varying mu and sigma.

``` {r}
mu <- 6
sigma <- 2
x <- seq(from=(mu - 4*sigma), to=(mu + 4*sigma), length.out = 1000)
fx <- dnorm(x,mean=mu,sd=sigma)
df <- as.data.frame(cbind(x,fx))
plot(df$x,df$fx,type="l",main=paste("N(mu=",mu,", sigma=",sigma, ") distribution", sep=""), xlab="x",ylab="f(x)")
abline(v=c(mu - 1*sigma, mu + 1 * sigma))
abline(h=0)

sd1 <- df[x>=(mu - 1*sigma) & x<=(mu + 1*sigma),]
sd2 <- df[x>=(mu - 2*sigma) & x<=(mu + 2*sigma),]

polygon(rbind(c(mu - 1*sigma,0),cbind(sd1$x,sd1$fx),c(mu + 1*sigma,0)),border=NA,col="gray")

```

The `qnorm()` function will tell us the value of $x$ below which a given proportion of the cumulative probability function falls. As we saw earlier, we can use `qnorm()` to calculate confidence intervals.

``` {r eval = FALSE}
abline(v=c(qnorm(0.025,mean=mu,sd=sigma),qnorm(0.975,mean=mu,sd=sigma)))

polygon(rbind(c(mu - 2*sigma,0),cbind(sd2$x,sd2$fx),c(mu + 2*sigma,0)),border=NA,col="gray")
```

#### CHALLENGE:

- Create a vector containing **N** random numbers selected from a normal distribution with mean **mu** and standard deviation **sigma**. Use 10000 for **N**, 3.5 for **mu**, and 4 for **sigma**. HINT: such a function exists! `rnorm()`
- Calculate the mean, variance, and SE in your set of random numbers.
- Plot a histogram of your random numbers

```{r}
N <- 10000
mu <- 3.5
sigma <- 4
v <- rnorm(N,mu,sigma)
mean(v)
var(v)
se <- sqrt(var(v)/N)
se
hist(v,breaks=seq(from=-15, to=20, by=0.5), probability = TRUE)
```
A quantile-quantile plot can be used to look at whether the 

``` {r}
qqnorm(v,main="Normal QQ plot random normal variables")
qqline(v,col="gray")
```

#### HOMEWORK (Continued)
Answer this question in the same `.Rmd` and `.html` file you started for Problem 1.

Problem 2:
